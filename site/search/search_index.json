{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cover","text":""},{"location":"Data/DataFrameworks/","title":"Data Frameworks","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataFrameworks/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataFrameworks/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataFrameworks/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataFrameworks/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataFrameworks/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataFrameworks/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Data/DataFundamentals/","title":"Data Fundamentals","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataFundamentals/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataFundamentals/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataFundamentals/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataFundamentals/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataFundamentals/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataFundamentals/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Data/DataServices/","title":"Data Services","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataServices/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataServices/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataServices/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataServices/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataServices/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataServices/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Foundation/","title":"OnRamp to modern Intelligent AI Solutions","text":"<p>While not a book about AI fundamentals, we want to provide an onramp for preparing you for crafting modern AI solutions.</p> <p>Resiliency is the ability of your system to react to failure and still remain functional. It's not about avoiding failure, but accepting failure and constructing your cloud-native services to respond to it. You want to return to a fully functioning state quickly as possible.</p> <p>Unlike traditional monolithic applications, where everything runs together in a single process, cloud-native systems embrace a distributed architecture as shown in Figure 6-1:</p> <p></p> <p>Figure 6-1. Distributed cloud-native environment</p> <p>In the previous figure, each microservice and cloud-based backing service execute in a separate process, across server infrastructure, communicating via network-based calls.</p> <p>Operating in this environment, a service must be sensitive to many different challenges:</p> <ul> <li> <p>Unexpected network latency - the time for a service request to travel to the receiver and back.</p> </li> <li> <p>Transient faults - short-lived network connectivity errors.</p> </li> <li> <p>Blockage by a long-running synchronous operation.</p> </li> <li> <p>A host process that has crashed and is being restarted or moved.</p> </li> <li> <p>An overloaded microservice that can't respond for a short time.</p> </li> <li> <p>An in-flight orchestrator operation such as a rolling upgrade or moving a service from one node to another.</p> </li> <li> <p>Hardware failures.</p> </li> </ul> <p>Cloud platforms can detect and mitigate many of these infrastructure issues. It may restart, scale out, and even redistribute your service to a different node.  However, to take full advantage of this built-in protection, you must design your services to react to it and thrive in this dynamic environment.</p> <p>In the following sections, we'll explore defensive techniques that your service and managed cloud resources can leverage to minimize downtime and disruption.</p>"},{"location":"Foundation/Advanced/","title":"Advanced","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the Advanced content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Advanced/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Advanced/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Advanced/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Advanced/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/Fundamentals/","title":"Fundamentals","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Fundamentals/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Fundamentals/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Fundamentals/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Fundamentals/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/GettingStarted/","title":"Getting Started","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the Getting Started content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/GettingStarted/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/GettingStarted/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/GettingStarted/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/GettingStarted/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Introduction/introduction/","title":"Introduction","text":"<p>Another day, at the office, working on \"the next big thing.\"</p> <p>Your cellphone rings. It's your friendly recruiter - the one who calls daily with exciting new opportunities.</p> <p>But this time it's different: Start-up, equity, and plenty of funding.</p> <p>The mention of the cloud, microservices, and cutting-edge technology pushes you over the edge.</p> <p>Fast forward a few weeks and you're now a new employee in a design session architecting a major eCommerce application. You're going to compete with the leading eCommerce sites.</p> <p>How will you build it?</p> <p>If you follow the guidance from past 15 years, you'll most likely build the system shown in Figure 1.1.</p> <p></p> <p>Figure 1-1. Traditional monolithic design</p> <p>You construct a large core application containing all of your domain logic. It includes modules such as Identity, Catalog, Ordering, and more. They directly communicate with each other within a single server process. The modules share a large relational database. The core exposes functionality via an HTML interface and a mobile app.</p> <p>Congratulations!  You just created a monolithic application.</p> <p>Not all is bad. Monoliths offer some distinct advantages. For example, they're straightforward to...</p> <ul> <li>build</li> <li>test</li> <li>deploy</li> <li>troubleshoot</li> <li>vertically scale</li> </ul> <p>Many successful apps that exist today were created as monoliths. The app is a hit and continues to evolve, iteration after iteration, adding more functionality.</p> <p>At some point, however, you begin to feel uncomfortable. You find yourself losing control of the application. As time goes on, the feeling becomes more intense, and you eventually enter a state known as the <code>Fear Cycle</code>:</p> <ul> <li>The app has become so overwhelmingly complicated that no single person understands it.</li> <li>You fear making changes - each change has unintended and costly side effects.</li> <li>New features/fixes become tricky, time-consuming, and expensive to implement.</li> <li>Each release becomes as small as possible and requires a full deployment of the entire application.</li> <li>One unstable component can crash the entire system.</li> <li>New technologies and frameworks aren't an option.</li> <li>It's difficult to implement agile delivery methodologies.</li> <li>Architectural erosion sets in as the code base deteriorates with never-ending \"quick fixes.\"</li> <li>Finally, the consultants come in and tell you to rewrite it.</li> </ul> <p>Sound familiar?</p> <p>Many organizations have addressed this monolithic fear cycle by adopting a cloud-native approach to building systems. Figure 1-2 shows the same system built applying cloud-native techniques and practices.</p> <p></p> <p>Figure 1-2. Cloud-native design</p> <p>Note how the application is decomposed across a set of small isolated microservices. Each service is self-contained and encapsulates its own code, data, and dependencies. Each is deployed in a software container and managed by a container orchestrator. Instead of a large relational database, each service owns it own datastore, the type of which vary based upon the data needs. Note how some services depend on a relational database, but other on NoSQL databases. One service stores its state in a distributed cache. Note how all traffic routes through an API Gateway service that is responsible for routing traffic to the core back-end services and enforcing many cross-cutting concerns. Most importantly, the application takes full advantage of the scalability, availability, and resiliency features found in modern cloud platforms.</p>"},{"location":"Introduction/introduction/#cloud-native-computing","title":"Cloud-native computing","text":"<p>Hmm... We just used the term, Cloud Native. Your first thought might be, \"What exactly does that mean?\" Another industry buzzword concocted by software vendors to market more stuff?\"</p> <p>Fortunately it's far different, and hopefully this book will help convince you.</p> <p>Within a short time, cloud native has become a driving trend in the software industry. It's a new way to construct large, complex systems. The approach takes full advantage of modern software development practices, technologies, and cloud infrastructure. Cloud native changes the way you design, implement, deploy, and operationalize systems.</p> <p>Unlike the continuous hype that drives our industry, cloud native is for-real. Consider the Cloud Native Computing Foundation (CNCF), a consortium of over 400 major corporations. Its charter is to make cloud-native computing ubiquitous across technology and cloud stacks. As one of the most influential open-source groups, it hosts many of the fastest-growing open source-projects in GitHub. These projects include Kubernetes, Prometheus, Helm, Envoy, and gRPC.</p> <p>The CNCF fosters an ecosystem of open-source and vendor-neutrality. Following that lead, this book presents cloud-native principles, patterns, and best practices that are technology agnostic. At the same time, we discuss the services and infrastructure available in the Microsoft Azure cloud for constructing cloud-native systems.</p> <p>So, what exactly is Cloud Native? Sit back, relax, and let us help you explore this new world.</p>"},{"location":"Other/Frameworks/Langchain/","title":"Langchain","text":""},{"location":"Other/Frameworks/Langchain/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Other/Frameworks/LlamaIndex/","title":"Llama Index","text":""},{"location":"Other/Frameworks/LlamaIndex/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Other/Frameworks/SemanticKernel/","title":"Semantic Kernel","text":""},{"location":"Other/Frameworks/SemanticKernel/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page2/","title":"Page 2","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page2/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page3/","title":"Page 3","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page3/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page4/","title":"Page 2","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page4/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"}]}