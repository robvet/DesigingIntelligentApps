{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Cover","text":""},{"location":"Data/DataFrameworks/","title":"Data Frameworks","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataFrameworks/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataFrameworks/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataFrameworks/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataFrameworks/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataFrameworks/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataFrameworks/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Data/DataFundamentals/","title":"Data Fundamentals","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataFundamentals/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataFundamentals/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataFundamentals/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataFundamentals/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataFundamentals/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataFundamentals/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Data/DataServices/","title":"Data Services","text":"<p>As we've seen throughout this book, a cloud-native approach changes the way you design, deploy, and manage applications. It also changes the way you manage and store data.</p> <p>Figure 5-1 contrasts the differences.</p> <p></p> <p>Figure 5-1. Data management in cloud-native applications</p> <p>Experienced developers will easily recognize the architecture on the left-side of figure 5-1. In this monolithic application, business service components collocate together in a shared services tier, sharing data from a single relational database.</p> <p>In many ways, a single database keeps data management simple. Querying data across multiple tables is straightforward. Changes to data update together or they all rollback. ACID transactions guarantee strong and immediate consistency.</p> <p>Designing for cloud-native, we take a different approach. On the right-side of Figure 5-1, note how business functionality segregates into small, independent microservices. Each microservice encapsulates a specific business capability and its own data. The monolithic database decomposes into a distributed data model with many smaller databases, each aligning with a microservice. When the smoke clears, we emerge with a design that exposes a database per microservice.</p>"},{"location":"Data/DataServices/#database-per-microservice-why","title":"Database-per-microservice, why?","text":"<p>This database per microservice provides many benefits, especially for systems that must evolve rapidly and support massive scale. With this model...</p> <ul> <li>Domain data is encapsulated within the service</li> <li>Data schema can evolve without directly impacting other services</li> <li>Each data store can independently scale</li> <li>A data store failure in one service won't directly impact other services</li> </ul> <p>Segregating data also enables each microservice to implement the data store type that is best optimized for its workload, storage needs, and read/write patterns. Choices include relational, document, key-value, and even graph-based data stores.</p> <p>Figure 5-2 presents the principle of polyglot persistence in a cloud-native system.</p> <p></p> <p>Figure 5-2. Polyglot data persistence</p> <p>Note in the previous figure how each microservice supports a different type of data store.</p> <ul> <li>The product catalog microservice consumes a relational database to accommodate the rich relational structure of its underlying data.</li> <li>The shopping cart microservice consumes a distributed cache that supports its simple, key-value data store.</li> <li>The ordering microservice consumes both a NoSql document database for write operations along with a highly denormalized key/value store to accommodate high-volumes of read operations.</li> </ul> <p>While relational databases remain relevant for microservices with complex data, NoSQL databases have gained considerable popularity. They provide massive scale and high availability. Their schemaless nature allows developers to move away from an architecture of typed data classes and ORMs that make change expensive and time-consuming. We cover NoSQL databases later in this chapter.</p> <p>While encapsulating  data into separate microservices can increase agility, performance, and scalability, it also presents many challenges. In the next section, we discuss these challenges along with patterns and practices to help overcome them.  </p>"},{"location":"Data/DataServices/#cross-service-queries","title":"Cross-service queries","text":"<p>While microservices are independent and focus on specific functional capabilities, like inventory, shipping, or ordering, they frequently require integration with other microservices. Often the integration involves one microservice querying another for data. Figure 5-3 shows the scenario.</p> <p></p> <p>Figure 5-3. Querying across microservices</p> <p>In the preceding figure, we see a shopping basket microservice that adds an item to a user's shopping basket. While the data store for this microservice contains basket and line item data, it doesn't maintain product or pricing data. Instead, those data items are owned by the catalog and pricing microservices. This aspect presents a problem. How can the shopping basket microservice add a product to the user's shopping basket when it doesn't have product nor pricing data in its database?</p> <p>One option discussed in Chapter 4 is a direct HTTP call from the shopping basket to the catalog and pricing microservices. However, in chapter 4, we said synchronous HTTP calls couple microservices together, reducing their autonomy and diminishing their architectural benefits.</p> <p>We could also implement a request-reply pattern with separate inbound and outbound queues for each service. However, this pattern is complicated and requires plumbing to correlate request and response messages. While it does decouple the backend microservice calls, the calling service must still synchronously wait for the call to complete. Network congestion, transient faults, or an overloaded microservice and can result in long-running and even failed operations.</p> <p>Instead, a widely accepted pattern for removing cross-service dependencies is the Materialized View Pattern, shown in Figure 5-4.</p> <p></p> <p>Figure 5-4. Materialized View Pattern</p> <p>With this pattern, you place a local data table (known as a read model) in the shopping basket service. This table contains a denormalized copy of the data needed from the product and pricing microservices. Copying the data directly into the shopping basket microservice eliminates the need for expensive cross-service calls. With the data local to the service, you improve the service's response time and reliability. Additionally, having its own copy of the data makes the shopping basket service more resilient. If the catalog service should become unavailable, it wouldn't directly impact the shopping basket service. The shopping basket can continue operating with the data from its own store.</p> <p>The catch with this approach is that you now have duplicate data in your system. However, strategically duplicating data in cloud-native systems is an established practice and not considered an anti-pattern, or bad practice. Keep in mind that one and only one service can own a data set and have authority over it. You'll need to synchronize the read models when the system of record is updated. Synchronization is typically implemented via asynchronous messaging with a publish/subscribe pattern, as shown in Figure 5.4.</p>"},{"location":"Data/DataServices/#distributed-transactions","title":"Distributed transactions","text":"<p>While querying data across microservices is difficult, implementing a transaction across several microservices is even more complex. The inherent challenge of maintaining data consistency across independent data sources in different microservices can't be understated. The lack of distributed transactions in cloud-native applications means that you must manage distributed transactions programmatically. You move from a world of immediate consistency to that of eventual consistency.</p> <p>Figure 5-5 shows the problem.</p> <p></p> <p>Figure 5-5. Implementing a transaction across microservices</p> <p>In the preceding figure, five independent microservices participate in a distributed transaction that creates an order. Each microservice maintains its own data store and implements a local transaction for its store. To create the order, the local transaction for each individual microservice must succeed, or all must abort and roll back the operation. While built-in transactional support is available inside each of the microservices, there's no support for a distributed transaction that would span across all five services to keep data consistent.</p> <p>Instead, you must construct this distributed transaction programmatically.</p> <p>A popular pattern for adding distributed transactional support is the Saga pattern. It's implemented by grouping local transactions together programmatically and sequentially invoking each one. If any of the local transactions fail, the Saga aborts the operation and invokes a set of compensating transactions. The compensating transactions undo the changes made by the preceding local transactions and restore data consistency. Figure 5-6 shows a failed transaction with the Saga pattern.</p> <p></p> <p>Figure 5-6. Rolling back a transaction</p> <p>In the previous figure, the Update Inventory operation has failed in the Inventory microservice. The Saga invokes a set of compensating transactions (in red) to adjust the inventory counts, cancel the payment and the order, and return the data for each microservice back to a consistent state.</p> <p>Saga patterns are typically choreographed as a series of related events, or orchestrated as a set of related commands. In Chapter 4, we discussed the service aggregator pattern that would be the foundation for an orchestrated saga implementation. We also discussed eventing along with Azure Service Bus and Azure Event Grid topics that would be a foundation for a choreographed saga implementation.</p>"},{"location":"Data/DataServices/#high-volume-data","title":"High volume data","text":"<p>Large cloud-native applications often support high-volume data requirements. In these scenarios, traditional data storage techniques can cause bottlenecks. For complex systems that deploy on a large scale, both Command and Query Responsibility Segregation (CQRS) and Event Sourcing may improve application performance.  </p>"},{"location":"Data/DataServices/#cqrs","title":"CQRS","text":"<p>CQRS, is an architectural pattern that can help maximize performance, scalability, and security. The pattern separates operations that read data from those operations that write data.</p> <p>For normal scenarios, the same entity model and data repository object are used for both read and write operations.</p> <p>However, a high volume data scenario can benefit from separate models and data tables for reads and writes. To improve performance, the read operation could query against a highly denormalized representation of the data to avoid expensive repetitive table joins and table locks. The write operation, known as a command, would update against a fully normalized representation of the data that would guarantee consistency. You then need to implement a mechanism to keep both representations in sync. Typically, whenever the write table is modified, it publishes an event that replicates the modification to the read table.</p> <p>Figure 5-7 shows an implementation of the CQRS pattern.</p> <p></p> <p>Figure 5-7. CQRS implementation</p> <p>In the previous figure, separate command and query models are implemented. Each data write operation is saved to the write store and then propagated to the read store. Pay close attention to how the data propagation process operates on the principle of eventual consistency. The read model eventually synchronizes with the write model, but there may be some lag in the process. We discuss eventual consistency in the next section.</p> <p>This separation enables reads and writes to scale independently. Read operations use a schema optimized for queries, while the writes use a schema optimized for updates. Read queries go against denormalized data, while complex business logic can be applied to the write model. As well, you might impose tighter security on write operations than those exposing reads.</p> <p>Implementing CQRS can improve application performance for cloud-native services. However, it does result in a more complex design. Apply this principle carefully and strategically to those sections of your cloud-native application that will benefit from it. For more on CQRS, see the Microsoft book .NET Microservices: Architecture for Containerized .NET Applications.</p>"},{"location":"Data/DataServices/#event-sourcing","title":"Event sourcing","text":"<p>Another approach to optimizing high volume data scenarios involves Event Sourcing.</p> <p>A system typically stores the current state of a data entity. If a user changes their phone number, for example, the customer record is updated with the new number. We always know the current state of a data entity, but each update overwrites the previous state.</p> <p>In most cases, this model works fine. In high volume systems, however, overhead from transactional locking and frequent update operations can impact database performance, responsiveness, and limit scalability.</p> <p>Event Sourcing takes a different approach to capturing data. Each operation that affects data is persisted to an event store. Instead of updating the state of a data record, we append each change to a sequential list of past events - similar to an accountant's ledger. The Event Store becomes the system of record for the data. It's used to propagate various materialized views within the bounded context of a microservice. Figure 5.8 shows the pattern.</p> <p></p> <p>Figure 5-8. Event Sourcing</p> <p>In the previous figure, note how each entry (in blue) for a user's shopping cart is appended to an underlying event store. In the adjoining materialized view, the system projects the current state by replaying all the events associated with each shopping cart. This view, or read model, is then exposed back to the UI. Events can also be integrated with external systems and applications or queried to determine the current state of an entity. With this approach, you maintain history. You know not only the current state of an entity, but also how you reached this state.</p> <p>Mechanically speaking, event sourcing simplifies the write model. There are no updates or deletes. Appending each data entry as an immutable event minimizes contention, locking, and concurrency conflicts associated with relational databases. Building read models with the materialized view pattern enables you to decouple the view from the write model and choose the best data store to optimize the needs of your application UI.</p> <p>For this pattern, consider a data store that directly supports event sourcing. Azure Cosmos DB, MongoDB, Cassandra, CouchDB, and RavenDB are good candidates.</p> <p>As with all patterns and technologies, implement strategically and when needed. While event sourcing can provide increased performance and scalability, it comes at the expense of complexity and a learning curve.</p>"},{"location":"Data/Modeling/","title":"Modeling Overview","text":""},{"location":"Data/Modeling/#overview-of-the-data-modeling-content","title":"Overview of the Data Modeling Content","text":""},{"location":"Data/Modeling/#index-page-for-onrampmodeling-sub-section","title":"Index Page for OnRamp/Modeling Sub Section","text":"<p>Resiliency is the ability of your system to react to failure and still remain functional. It's not about avoiding failure, but accepting failure and constructing your cloud-native services to respond to it. You want to return to a fully functioning state quickly as possible.</p> <p>Unlike traditional monolithic applications, where everything runs together in a single process, cloud-native systems embrace a distributed architecture as shown in Figure 6-1:</p> <p></p> <p>Figure 6-1. Distributed cloud-native environment</p> <p>In the previous figure, each microservice and cloud-based backing service execute in a separate process, across server infrastructure, communicating via network-based calls.</p> <p>Operating in this environment, a service must be sensitive to many different challenges:</p> <ul> <li> <p>Unexpected network latency - the time for a service request to travel to the receiver and back.</p> </li> <li> <p>Transient faults - short-lived network connectivity errors.</p> </li> <li> <p>Blockage by a long-running synchronous operation.</p> </li> <li> <p>A host process that has crashed and is being restarted or moved.</p> </li> <li> <p>An overloaded microservice that can't respond for a short time.</p> </li> <li> <p>An in-flight orchestrator operation such as a rolling upgrade or moving a service from one node to another.</p> </li> <li> <p>Hardware failures.</p> </li> </ul> <p>Cloud platforms can detect and mitigate many of these infrastructure issues. It may restart, scale out, and even redistribute your service to a different node.  However, to take full advantage of this built-in protection, you must design your services to react to it and thrive in this dynamic environment.</p> <p>In the following sections, we'll explore defensive techniques that your service and managed cloud resources can leverage to minimize downtime and disruption.</p>"},{"location":"Data/Modeling/NoSql/","title":"NoSql","text":""},{"location":"Data/Modeling/NoSql/#subsection-for-nosql-modeling","title":"Subsection for NoSql Modeling","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Data/Modeling/NoSql/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Data/Modeling/NoSql/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Data/Modeling/NoSql/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Data/Modeling/NoSql/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Data/Modeling/Relational/","title":"Relational","text":""},{"location":"Data/Modeling/Relational/#subsection-for-relational-modeling","title":"Subsection for Relational Modeling","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Data/Modeling/Relational/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Data/Modeling/Relational/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Data/Modeling/Relational/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Data/Modeling/Relational/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Designs/Orchestration/","title":"Orchestration Overview","text":""},{"location":"Designs/Orchestration/#overview-of-the-orchestration-content","title":"Overview of the Orchestration Content","text":""},{"location":"Designs/Orchestration/#index-page-for-onrampmodeling-sub-section","title":"Index Page for OnRamp/Modeling Sub Section","text":"<p>Up until now, we could build an orchestration in Copilot Studio to reliably execute a predefined path of tasks. With a similar experience to Logic Apps, it's a visual, no-code experience with connectors and the ability to invoke custom code.</p> <p>We could also build a custom LLM app in which we can feed a prompt, business rules, and data, and have the LLM reason through the orchestration steps. For a more deterministic experience, we'd want to build an LLM agent with tools (functions) for each step. The LLM agent can reason through the orchestration and invoke the needed tools.   </p> <p>At initial glance, the Semantic Kernel Process Framework appears to sit in the middel. </p> <p>construct as a no-code alternative for building process flows and contrasting it with building an agent workflow that can infer processes and call on needed tools (or OpenAI Functions). </p> <p>Resiliency is the ability of your system to react to failure and still remain functional. It's not about avoiding failure, but accepting failure and constructing your cloud-native services to respond to it. You want to return to a fully functioning state quickly as possible.</p> <p>Unlike traditional monolithic applications, where everything runs together in a single process, cloud-native systems embrace a distributed architecture as shown in Figure 6-1:</p> <p></p> <p>Figure 6-1. Distributed cloud-native environment</p> <p>Operating in this environment, a service must be sensitive to many different challenges:</p> <ul> <li> <p>Unexpected network latency - the time for a service request to travel to the receiver and back.</p> </li> <li> <p>Transient faults - short-lived network connectivity errors.</p> </li> <li> <p>Blockage by a long-running synchronous operation.</p> </li> </ul>"},{"location":"Designs/Orchestration/Orchestration/","title":"Orchestration","text":""},{"location":"Designs/Orchestration/Orchestration/#subsection-for-design","title":"Subsection for Design","text":"<p>Up until now, we could build an orchestration in Copilot Studio to reliably execute a predefined path of tasks. With a similar experience to Logic Apps, it's a visual, no-code experience with connectors and the ability to invoke custom code.</p> <p>We could also build a custom LLM app in which we can feed a prompt, business rules, and data, and have the LLM reason through the orchestration steps. For a more deterministic experience, we'd want to build an LLM agent with tools (functions) for each step. The LLM agent can reason through the orchestration and invoke the needed tools.   </p> <p>At initial glance, the Semantic Kernel Process Framework appears to sit in the middel. </p> <p>construct as a no-code alternative for building process flows and contrasting it with building an agent workflow that can infer processes and call on needed tools (or OpenAI Functions). </p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul>"},{"location":"Designs/Orchestration/Orchestration/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p>"},{"location":"Designs/Orchestration/Orchestration/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p>"},{"location":"Designs/Orchestration/Orchestration/#azure-api-management","title":"Azure API Management","text":"<p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul>"},{"location":"Foundation/","title":"The OnRamp to Modern Intelligent AI Solutions","text":"<p>While not a book about AI fundamentals, we do want to provide an onramp to help prepare you for crafting modern AI solutions.</p> <p>Blah, blah, blah</p> <p>Embrace modern intelligent apps as shown in Figure 6-1:</p> <p></p> <p>Figure 6-1. Distributed cloud-native environment</p> <p>Blah, blah, blah</p> <p>In the following sections, we'll explore fundamental concept, techniques, and technologies you'll need to understand to propel yourself into this exciting journey.</p>"},{"location":"Foundation/Advanced/","title":"Advanced","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the Advanced content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Advanced/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Advanced/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Advanced/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Advanced/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/Fundamentals/","title":"Fundamentals","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Fundamentals/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Fundamentals/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Fundamentals/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Fundamentals/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/GettingStarted/","title":"Getting Started","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the Getting Started content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/GettingStarted/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/GettingStarted/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/GettingStarted/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/GettingStarted/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/Fundamentals/","title":"Fundamental Overview","text":""},{"location":"Foundation/Fundamentals/#overview-of-the-onramp-fundamentals-content","title":"Overview of the OnRamp Fundamentals Content","text":""},{"location":"Foundation/Fundamentals/#index-page-for-onrampfundamentals-sub-section","title":"Index Page for OnRamp/Fundamentals Sub Section","text":"<p>Resiliency is the ability of your system to react to failure and still remain functional. It's not about avoiding failure, but accepting failure and constructing your cloud-native services to respond to it. You want to return to a fully functioning state quickly as possible.</p> <p>Unlike traditional monolithic applications, where everything runs together in a single process, cloud-native systems embrace a distributed architecture as shown in Figure 6-1:</p> <p></p> <p>Figure 6-1. Distributed cloud-native environment</p> <p>In the previous figure, each microservice and cloud-based backing service execute in a separate process, across server infrastructure, communicating via network-based calls.</p> <p>Operating in this environment, a service must be sensitive to many different challenges:</p> <ul> <li> <p>Unexpected network latency - the time for a service request to travel to the receiver and back.</p> </li> <li> <p>Transient faults - short-lived network connectivity errors.</p> </li> <li> <p>Blockage by a long-running synchronous operation.</p> </li> <li> <p>A host process that has crashed and is being restarted or moved.</p> </li> <li> <p>An overloaded microservice that can't respond for a short time.</p> </li> <li> <p>An in-flight orchestrator operation such as a rolling upgrade or moving a service from one node to another.</p> </li> <li> <p>Hardware failures.</p> </li> </ul> <p>Cloud platforms can detect and mitigate many of these infrastructure issues. It may restart, scale out, and even redistribute your service to a different node.  However, to take full advantage of this built-in protection, you must design your services to react to it and thrive in this dynamic environment.</p> <p>In the following sections, we'll explore defensive techniques that your service and managed cloud resources can leverage to minimize downtime and disruption.</p>"},{"location":"Foundation/Fundamentals/Numerus/","title":"Numerus","text":""},{"location":"Foundation/Fundamentals/Numerus/#subsection-for-fundamentals","title":"Subsection for Fundamentals","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Fundamentals/Numerus/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Fundamentals/Numerus/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Fundamentals/Numerus/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Fundamentals/Numerus/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"Foundation/Fundamentals/Verbum/","title":"Verbum","text":""},{"location":"Foundation/Fundamentals/Verbum/#subsection-for-fundamentals","title":"Subsection for Fundamentals","text":"<p>In a cloud-native system, front-end clients (mobile, web, and desktop applications) require a communication channel to interact with independent back-end microservices.</p> <p>This is the fundamentals content.</p> <p>What are the options?</p> <p>To keep things simple, a front-end client could directly communicate with the back-end microservices, shown in Figure 4-2.</p> <p></p> <p>Figure 4-2. Direct client to service communication</p> <p>With this approach, each microservice has a public endpoint that is accessible by front-end clients. In a production environment, you'd place a load balancer in front of the microservices, routing traffic proportionately.</p> <p>While simple to implement, direct client communication would be acceptable only for simple microservice applications. This pattern tightly couples front-end clients to core back-end services, opening the door for many problems, including:</p> <ul> <li>Client susceptibility to back-end service refactoring.</li> <li>A wider attack surface as core back-end services are directly exposed.</li> <li>Duplication of cross-cutting concerns across each microservice.</li> <li>Overly complex client code - clients must keep track of multiple endpoints and handle failures in a resilient way.</li> </ul> <p>Instead, a widely accepted cloud design pattern is to implement an API Gateway Service between the front-end applications and back-end services. The pattern is shown in Figure 4-3.</p> <p></p> <p>Figure 4-3. API gateway pattern</p> <p>In the previous figure, note how the API Gateway service abstracts the back-end core microservices. Implemented as a web API, it acts as a reverse proxy, routing incoming traffic to the internal microservices.</p> <p>The gateway insulates the client from internal service partitioning and refactoring. If you change a back-end service, you accommodate for it in the gateway without breaking the client. It's also your first line of defense for cross-cutting concerns, such as identity, caching, resiliency, metering, and throttling. Many of these cross-cutting concerns can be off-loaded from the back-end core services to the gateway, simplifying the back-end services.</p> <p>Care must be taken to keep the API Gateway simple and fast. Typically, business logic is kept out of the gateway. A complex gateway risks becoming a bottleneck and eventually a monolith itself. Larger systems often expose multiple API Gateways segmented by client type (mobile, web, desktop) or back-end functionality. The Backend for Frontends pattern provides direction for implementing multiple gateways. The pattern is shown in Figure 4-4.</p> <p></p> <p>Figure 4-4. Backend for frontend pattern</p> <p>Note in the previous figure how incoming traffic is sent to a specific API gateway - based upon client type: web, mobile, or desktop app. This approach makes sense as the capabilities of each device differ significantly across form factor, performance, and display limitations. Typically mobile applications expose less functionality than a browser or desktop applications. Each gateway can be optimized to match the capabilities and functionality of the corresponding device.</p>"},{"location":"Foundation/Fundamentals/Verbum/#simple-gateways","title":"Simple Gateways","text":"<p>To start, you could build your own API Gateway service. A quick search of GitHub will provide many examples.</p> <p>For simple .NET cloud-native applications, you might consider the Ocelot Gateway. Open source and created for .NET microservices, it's lightweight, fast, scalable. Like any API Gateway, its primary functionality is to forward incoming HTTP requests to downstream services. Additionally, it supports a wide variety of capabilities that are configurable in a .NET middleware pipeline.</p> <p>YARP (Yet Another Reverse proxy) is another open source reverse proxy led by a group of Microsoft product teams. Downloadable as a NuGet package, YARP plugs into the ASP.NET framework as middleware and is highly customizable. You'll find YARP well-documented with various usage examples.</p> <p>For enterprise cloud-native applications, there are several managed Azure services that can help jump-start your efforts.</p>"},{"location":"Foundation/Fundamentals/Verbum/#azure-application-gateway","title":"Azure Application Gateway","text":"<p>For simple gateway requirements, you may consider Azure Application Gateway. Available as an Azure PaaS service, it includes basic gateway features such as URL routing, SSL termination, and a Web Application Firewall. The service supports Layer-7 load balancing capabilities. With Layer 7, you can route requests based on the actual content of an HTTP message, not just low-level TCP network packets.</p> <p>Throughout this book, we evangelize hosting cloud-native systems in Kubernetes. A container orchestrator, Kubernetes automates the deployment, scaling, and operational concerns of containerized workloads. Azure Application Gateway can be configured as an API gateway for Azure Kubernetes Service cluster.</p> <p>The Application Gateway Ingress Controller enables Azure Application Gateway to work directly with Azure Kubernetes Service. Figure 4.5 shows the architecture.</p> <p></p> <p>Figure 4-5. Application Gateway Ingress Controller</p> <p>Kubernetes includes a built-in feature that supports HTTP (Level 7) load balancing, called Ingress. Ingress defines a set of rules for how microservice instances inside AKS can be exposed to the outside world. In the previous image, the ingress controller interprets the ingress rules configured for the cluster and automatically configures the Azure Application Gateway. Based on those rules, the Application Gateway routes traffic to microservices running inside AKS. The ingress controller listens for changes to ingress rules and makes the appropriate changes to the Azure Application Gateway.</p>"},{"location":"Foundation/Fundamentals/Verbum/#azure-api-management","title":"Azure API Management","text":"<p>For moderate to large-scale cloud-native systems, you may consider Azure API Management. It's a cloud-based service that not only solves your API Gateway needs, but provides a full-featured developer and administrative experience. API Management is shown in Figure 4-6.</p> <p></p> <p>Figure 4-6. Azure API Management</p> <p>To start, API Management exposes a gateway server that allows controlled access to back-end services based upon configurable rules and policies. These services can be in the Azure cloud, your on-prem data center, or other public clouds. API keys and JWT tokens determine who can do what. All traffic is logged for analytical purposes.</p> <p>For developers, API Management offers a developer portal that provides access to services, documentation, and sample code for invoking them. Developers can use Swagger/Open API to inspect service endpoints and analyze their usage. The service works across the major development platforms: .NET, Java, Golang, and more.</p> <p>The publisher portal exposes a management dashboard where administrators expose APIs and manage their behavior. Service access can be granted, service health monitored, and service telemetry gathered. Administrators apply policies to each endpoint to affect behavior. Policies are pre-built statements that execute sequentially for each service call.  Policies are configured for an inbound call, outbound call, or invoked upon an error. Policies can be applied at different service scopes as to enable deterministic ordering when combining policies. The product ships with a large number of prebuilt policies.</p> <p>Here are examples of how policies can affect the behavior of your cloud-native services:  </p> <ul> <li>Restrict service access.</li> <li>Enforce authentication.  </li> <li>Throttle calls from a single source, if necessary.</li> <li>Enable caching.</li> <li>Block calls from specific IP addresses.</li> <li>Control the flow of the service.</li> <li>Convert requests from SOAP to REST or between different data formats, such as from XML to JSON.</li> </ul> <p>Azure API Management can expose back-end services that are hosted anywhere \u2013 in the cloud or your data center. For legacy services that you may expose in your cloud-native systems, it supports both REST and SOAP APIs. Even other Azure services can be exposed through API Management. You could place a managed API on top of an Azure backing service like Azure Service Bus or Azure Logic Apps. Azure API Management doesn't include built-in load-balancing support and should be used in conjunction with a load-balancing service.</p> <p>Azure API Management is available across four different tiers:</p> <ul> <li>Developer</li> <li>Basic</li> <li>Standard</li> <li>Premium</li> </ul> <p>The Developer tier is meant for non-production workloads and evaluation. The other tiers offer progressively more power, features, and higher service level agreements (SLAs). The Premium tier provides Azure Virtual Network and multi-region support. All tiers have a fixed price per hour.</p> <p>The Azure cloud also offers a serverless tier for Azure API Management. Referred to as the consumption pricing tier, the service is a variant of API Management designed around the serverless computing model. Unlike the \"pre-allocated\" pricing tiers previously shown, the consumption tier provides  instant provisioning and pay-per-action pricing.</p> <p>It enables API Gateway features for the following use cases:</p> <ul> <li>Microservices implemented using serverless technologies such as Azure Functions and Azure Logic Apps.</li> <li>Azure backing service resources such as Service Bus queues and topics, Azure storage, and others.</li> <li>Microservices where traffic has occasional large spikes but remains low most the time.</li> </ul> <p>The consumption tier uses the same underlying service API Management components, but employs an entirely different architecture based on dynamically allocated resources. It aligns perfectly with the serverless computing model:</p> <ul> <li>No infrastructure to manage.</li> <li>No idle capacity.</li> <li>High-availability.</li> <li>Automatic scaling.</li> <li>Cost is based on actual usage.</li> </ul> <p>The new consumption tier is a great choice for cloud-native systems that expose serverless resources as APIs.</p>"},{"location":"Foundation/Fundamentals/Verbum/#real-time-communication","title":"Real-time communication","text":"<p>Real-time, or push, communication is another option for front-end applications that communicate with back-end cloud-native systems over HTTP. Applications, such as financial-tickers, online education, gaming, and job-progress updates, require instantaneous, real-time responses from the back-end. With normal HTTP communication, there's no way for the client to know when new data is available. The client must continually poll or send requests to the server. With real-time communication, the server can push new data to the client at any time.</p> <p>Real-time systems are often characterized by high-frequency data flows and large numbers of concurrent client connections. Manually implementing real-time connectivity can quickly become complex, requiring non-trivial infrastructure to ensure scalability and reliable messaging to connected clients. You could find yourself managing an  instance of Azure Redis Cache and a set of load balancers configured with sticky sessions for client affinity.</p> <p>Azure SignalR Service is a fully managed Azure service that simplifies real-time communication for your cloud-native applications. Technical implementation details like capacity provisioning, scaling, and persistent connections are abstracted away. They're handled for you with a 99.9% service-level agreement. You focus on application features, not infrastructure plumbing.</p> <p>Once enabled, a cloud-based HTTP service can push content updates directly to connected clients, including browser, mobile and desktop applications. Clients are updated without the need to poll the server. Azure SignalR abstracts the transport technologies that create real-time connectivity, including WebSockets, Server-Side Events, and Long Polling. Developers focus on sending messages to all or specific subsets of connected clients.</p> <p>Figure 4-7 shows a set of HTTP Clients connecting to a Cloud-native application with Azure SignalR enabled.</p> <p></p> <p>Figure 4-7. Azure SignalR</p> <p>Another advantage of Azure SignalR Service comes with implementing Serverless cloud-native services. Perhaps your code is executed on demand with Azure Functions triggers. This scenario can be tricky because your code doesn't maintain long connections with clients. Azure SignalR Service can handle this situation since the service already manages connections for you.</p> <p>Azure SignalR Service closely integrates with other Azure services, such as Azure SQL Database, Service Bus, or Redis Cache, opening up many possibilities for your cloud-native applications.</p>"},{"location":"IntelligentOrchestration/AIAutomation/","title":"AI-Based Automation","text":"<p>Coming soon!</p> <p></p>"},{"location":"IntelligentOrchestration/AgentAutomation/","title":"Agent-Based Automation","text":"<p>Coming soon!</p> <p></p>"},{"location":"IntelligentOrchestration/Bots/","title":"Bots","text":"<p>Coming soon!</p> <p></p>"},{"location":"IntelligentOrchestration/Bots/#raw-bot-content","title":"Raw BOT Content","text":"<pre><code>A \"bot\" is a software application that automates tasks or simulates human \nconversations. In the context of the Azure AI stack, a bot fits into the \ncategory of Conversational AI. Specifically, bots are integrated with \nservices like Azure Bot Service and Azure Cognitive Services to enable \nintelligent interactions with users.\n\nHere's where a bot fits into the Azure AI stack:\nAzure Bot Service: This is the primary service for building bots on Azure. \nIt provides a framework for designing, building, testing, and deploying \nbots that can interact with users through a variety of channels, such as \nMicrosoft Teams, Slack, websites, and others.\n\nAzure Cognitive Services: Bots often use various Cognitive Services to \nenhance their capabilities. These services allow bots to:\n\n- Understand Language: Using services like Language Understanding (LUIS) \n  or Azure OpenAI, bots can process natural language inputs and respond \n  appropriately.\n- Speech Services: Convert speech to text and vice versa to allow \n  voice-based interactions.\n- Computer Vision: Allow bots to analyze and understand visual content \n  (e.g., image recognition).\n- Azure AI Solutions: Bots can leverage solutions such as Azure OpenAI \n  Service to use powerful large language models for generating human-like \n  responses, providing more natural and contextually aware interactions.\n\nIntegration with Other Azure Services: Bots are often integrated with \nother Azure services like Azure Functions for serverless compute, Azure \nLogic Apps for workflows, or Azure SQL for storing user interaction data.\n\nIn short, a bot in Azure is typically the front-facing component that \ninteracts with users, while services like Azure Bot Service and Cognitive \nServices provide the intelligence, language understanding, and interaction \ncapabilities.\n\nA bot in Azure often integrates workflows with cognitive capabilities to deliver intelligent, dynamic interactions. Here's how that breaks down:\n\n1 Workflow Integration:\n- Bots can trigger workflows to automate tasks and processes. These workflows are often built using tools like Azure Logic Apps or Azure Functions, allowing the bot to perform complex operations in response to user input. For instance, a bot could help a user check the status of an order, create an appointment, or perform backend operations like querying a database or sending notifications.\n1 Cognitive Capabilities:\n- The bot utilizes Cognitive Services (such as LUIS, Speech Services, Azure OpenAI, and more) to understand and respond to user input in a human-like way. These capabilities allow the bot to:\n  - Underrstand natural language through AI models (e.g., LUIS or OpenAI). \n  - Analyze user sentiment, detect objects in images, or even translate languages.\n  - Respond using text or voice, simulating human conversation.\n\nKey Benefits:\n- Dynamic Conversations: Bots can engage in complex, multi-turn conversations, handling contextual dialogue.\n- Automated Task Execution: They can automate a series of actions or invoke workflows (e.g., fetching information, scheduling, updating records).\n- Seamless Integration: Bots are often integrated into existing applications and platforms, providing a conversational interface to interact with services and data.\n\nIn essence, bots act as the user-facing component that seamlessly integrates workflows (for task automation) with AI-driven cognitive capabilities (for understanding and responding intelligently).\n\n\n\nSo, speaking precisely, let\u2019s define a workflow: A set of tasks or steps that execute a process?\n\nDefinition:\nA workflow is a set of tasks or steps that are executed in a specific sequence to complete a process or achieve a particular outcome.\nKey Aspects of a Workflow:\nTasks/Steps: Individual actions that make up the workflow. Each step can represent a distinct operation or decision point.\n\nExecution Sequence: The order in which tasks are executed. This can be linear (one task after another) or conditional (branching based on decisions or conditions).\n\nProcess: The overarching goal or operation that the workflow is designed to achieve, such as processing an order, onboarding a user, or handling a customer service request.\n\nAutomation: Workflows can be manual, automated, or a combination of both. In the case of bots, they are typically automated workflows triggered by user input or system events.\n\nExample:\nA customer support bot workflow might look like:\n\nReceive User Input: The bot asks the user for a query or issue description.\nAnalyze Input: The bot uses AI (e.g., natural language processing) to understand the issue.\nTrigger Action: The bot triggers a workflow, such as creating a support ticket, providing troubleshooting steps, or transferring to a live agent.\nFollow-Up: The bot confirms the resolution with the user or tracks the ticket status.\nIn short, a workflow represents a series of connected tasks that move data or operations through a defined path to complete a larger process.\n\n\nYes, the input to a bot workflow can indeed come from various sources, but most commonly, it is user input through text or speech. Here\u2019s how it works in more detail:\n\nTypes of Input:\nText Input:\n\nUsers type messages or commands, which the bot then processes using natural language understanding (NLU) services like LUIS or Azure OpenAI. For example, a user might type, \u201cWhat\u2019s the status of my order?\u201d and the bot parses this to trigger the appropriate workflow.\nSpeech Input:\n\nUsers speak to the bot, and the bot uses Speech-to-Text services to convert spoken language into text, then processes it in a similar way to text input. For example, the user could say, \u201cCheck my order status,\u201d and the bot would convert the speech into text and analyze it to proceed with the workflow.\nAdditional Input Channels:\nClick-based or Menu Inputs: In some cases, bots can accept inputs via pre-defined buttons or menus, often seen in chatbot interfaces where users choose from specific options.\n\nSensor or System Inputs: In more advanced workflows, input could come from external systems or sensors, where the bot reacts to data changes or system events rather than direct user interaction (though this is less common in conversational bots).\n\nProcessing:\nOnce the input is received (whether text or speech), it is analyzed and understood using AI models. Based on this understanding, the bot can decide which workflow to execute and what specific actions need to be taken.\n\nIn summary, the most common inputs to a bot are user inputs via text or speech, which trigger workflows to complete tasks or provide responses.\n\n\n\n\n\nanother process can certainly invoke a workflow without a bot being involved. In such cases, a bot is not necessarily required. Let me clarify the distinction:\n\nWhen a Bot is Needed:\nA bot is typically used when user interaction is involved, particularly when the user communicates via text or speech. Bots provide a conversational interface to trigger workflows based on user input, making them essential in situations where:\n\nUsers need to interact with a system conversationally.\nReal-time, human-like responses are expected (e.g., customer service, virtual assistants).\nWhen a Bot is Not Needed:\nIf the workflow is being triggered by another process, system, or event\u2014without direct user interaction\u2014then a bot is not necessary. Instead, you might simply use:\n\nAzure Logic Apps or Azure Functions to trigger workflows based on specific conditions, such as:\nEvent-driven workflows (e.g., an order placed in an e-commerce system).\nTime-based workflows (e.g., scheduling tasks at specific intervals).\nSystem-to-system workflows (e.g., integration between backend systems).\nIn this case, the workflow could be triggered automatically without the need for conversational input, and the bot layer wouldn\u2019t be required.\n\nExample without a Bot:\nLet\u2019s say you have a system where:\n\nA customer places an order, which automatically triggers a backend workflow to process payments, update inventory, and notify shipping.\nThis entire process can be automated using Azure Functions or Logic Apps, where the workflow starts based on the order placement event\u2014no bot is needed.\nWhen Both Are Useful:\nThere are scenarios where a bot and automated processes can coexist:\n\nUser-driven Bot Interaction: A user asks the bot, \"What's my order status?\"\nSystem-driven Workflow: Behind the scenes, the bot triggers a backend workflow to retrieve order information from a database or ERP system.\nIn summary, a bot is only needed when human interaction is a core requirement. If the workflow is purely process- or event-driven, the bot can be bypassed, and other automation tools (e.g., Logic Apps, Functions) can handle the workflow execution.\n\n\n\n\nThe role of the bot is to serve as the interface between the process (or workflow) and the user. The bot takes user input (through text, speech, or even menu-based selections), processes it, and then triggers the appropriate workflow behind the scenes. Once the workflow completes, the bot communicates the results or outcomes back to the user in a conversational manner.\n\nKey Roles of the Bot:\nUser Interaction: The bot handles the communication with the user, understanding their intent using AI services and natural language processing.\n\nTriggering Workflows: Based on the user\u2019s input, the bot triggers workflows or processes in the backend (e.g., querying a database, performing a transaction, or fetching data from another system).\n\nProviding Responses: Once the workflow completes, the bot sends the response back to the user in a friendly, understandable format, making the interaction feel seamless.\n\nIn essence, the bot bridges the gap between the user and the complex processes happening in the backend, providing a conversational front-end to otherwise technical or automated systems.\n\n\n\nIs the bot part of the workflow or more of an interface to interact with the workflow?\n\nThe bot is primarily the interface that interacts with the workflow rather than being a part of the workflow itself. Here's the distinction:\n\nBot as an Interface:\nThe bot serves as the user-facing layer. It is the component that receives user input (text, speech, etc.), processes it, and then determines what action to take based on the input.\nAfter understanding the user's intent (through natural language processing), the bot triggers or interacts with a backend workflow (which could be an automation process, a query, or an external service).\nThink of the bot as a messenger: it gathers information from the user and hands it over to the workflow, then returns the result or response back to the user.\nWorkflow as the Backend Process:\nThe workflow is the series of tasks or operations that actually executes the business logic or process. It could involve accessing databases, performing calculations, or interacting with other systems.\nFor example, once the bot understands that a user is asking for their order status, it triggers a workflow that:\nQueries a database for the order information.\nFormats the data for the bot to present.\nReturns the result to the bot, which then responds to the user.\nIn Summary:\nThe bot is not part of the workflow; rather, it is an interface that triggers the workflow and communicates the results of the workflow back to the user.\nThe workflow handles the actual logic and tasks behind the scenes, such as fetching data, processing requests, or integrating with other systems.\nThis separation of roles allows the bot to focus on the user interaction while the workflow focuses on the business process execution.\n\n\n\n\n</code></pre>"},{"location":"IntelligentOrchestration/Bots/#copilot-studio-raw-content","title":"Copilot Studio Raw Content","text":"<pre><code>Azure Copilot Studio provides both natural language capabilities and a visual platform for constructing workflows, making it a powerful tool for building conversational bots quickly.\n\nKey Features:\nNatural Language Capabilities:\n\nAzure Cognitive Services (like LUIS) are integrated, enabling the bot to understand and interpret user inputs in natural language. You can easily define intents (what the user wants) and entities (key pieces of information) without needing to code these models yourself.\nThis allows the bot to carry out dynamic, human-like conversations by responding intelligently to user queries.\nVisual Workflow Construction:\n\nCopilot Studio provides a drag-and-drop interface that allows users to visually design the bot\u2019s workflow. You can create a flow of tasks that the bot will follow based on user input or system triggers.\nThe visual interface also lets you easily integrate with backend workflows (via Power Automate) to connect the bot to business processes, databases, APIs, and other services. For example, you can add a workflow to check the status of an order or update a user\u2019s account.\nSummary:\nNatural Language Capabilities: Understand and respond to user input conversationally.\nVisual Workflow Construction: Build workflows that define how the bot responds, integrates with external services, and handles backend processes, all with minimal coding.\nIn short, Azure Copilot Studio allows you to both harness natural language understanding and easily construct workflows through a visual platform.\n\nThen Copilot Studio enables construction of the bot (user interface layer) and construction of the backend process?\n\nAzure Copilot Studio enables the construction of both the bot (user interface layer) and the integration of the bot with the backend process (workflow). Here\u2019s how it breaks down:\n\n1. Bot (User Interface Layer) Construction:\nIn Copilot Studio, you can build the bot's conversational interface using a visual, drag-and-drop interface. This includes:\nDefining conversations: You design how the bot interacts with the user, what questions it asks, and how it responds to user input.\nNatural language understanding: You can leverage built-in AI capabilities (like LUIS or Azure OpenAI) to interpret user queries, extract intents, and handle complex, multi-turn dialogues.\nMulti-channel deployment: The bots you create can be deployed across multiple platforms, like websites, Microsoft Teams, Slack, or custom apps.\n2. Backend Process (Workflow) Integration:\nOnce the bot understands the user's input, Copilot Studio allows you to integrate with Power Automate or other backend services to construct and trigger the workflow that handles the actual business logic. This includes:\nConnecting to external services: Through Power Automate, the bot can trigger automated workflows that access data from databases, call APIs, or integrate with business systems (e.g., CRM, ERP).\nExecuting tasks: These workflows can perform a variety of actions, such as processing orders, fetching user information, or sending notifications.\nReturning results: After the workflow completes, the results are sent back to the bot, which then relays the information to the user.\nIn Summary:\nCopilot Studio provides the tools to build the bot as the user interface.\nIt also enables you to construct and integrate backend workflows through visual tools like Power Automate.\nThis dual capability makes it possible to quickly build a fully-functional bot that not only interacts with users but also automates backend processes without the need for deep coding expertise.\n\n\n   o, Power Automate is the tool for building the workflow. How is it different from azure logic apps?\n\nPower Automate is the tool for building workflows, and it's similar to Azure Logic Apps in that both are used to automate processes. However, there are key differences between the two, especially in terms of target audience, features, and integration within the Microsoft ecosystem.\n\nKey Differences Between Power Automate and Azure Logic Apps:\n1. Target Audience:\nPower Automate: Primarily designed for business users and citizen developers. It\u2019s part of the Microsoft Power Platform and focuses on providing a no-code/low-code solution for automating everyday tasks and processes. It's integrated with Microsoft 365 apps, so it's commonly used to automate workflows within Office 365, SharePoint, Dynamics 365, and other business tools.\nAzure Logic Apps: Primarily aimed at developers and IT professionals. It\u2019s a part of the Azure ecosystem and is built for enterprise-grade workflows that require more complex integrations with Azure services, on-premises systems, and third-party APIs.\n2. Platform and Environment:\nPower Automate: Operates within the Power Platform and is more tightly integrated with business applications like Microsoft 365, Dynamics 365, and Power Apps. It allows users to quickly automate business processes without extensive Azure knowledge.\nAzure Logic Apps: Is part of Azure, providing a more scalable and flexible platform that integrates deeply with Azure services (like Azure Functions, Cosmos DB, and more). It\u2019s built for scenarios that require advanced automation, high scalability, and tighter integration with other Azure tools.\n3. Development Experience:\nPower Automate: Offers a simplified interface tailored for non-developers. It\u2019s ideal for automating simple, everyday business workflows (e.g., sending approval emails, creating alerts, moving files).\nAzure Logic Apps: Provides more control and flexibility for advanced workflows, including support for custom connectors, complex control flows, and more powerful integrations. Developers can work in Visual Studio and use Logic Apps in infrastructure-as-code deployments via ARM templates.\n4. Integration Options:\nPower Automate: Is better suited for business application workflows (like automating tasks in SharePoint, Teams, Outlook, or other Microsoft 365 apps). It also integrates with hundreds of SaaS services via pre-built connectors.\nAzure Logic Apps: Offers a broader range of integration options, including Azure-native services, on-premises systems, B2B workflows, and enterprise-grade APIs. It\u2019s more suitable for complex workflows that require high availability, error handling, and integration with non-Microsoft systems.\n5. Pricing Model:\nPower Automate: Has a per-user, per-flow pricing model, meaning pricing is based on the number of users and flows you create. It's usually more affordable for smaller, user-specific automations.\nAzure Logic Apps: Follows a pay-as-you-go pricing model, where you're charged based on the number of actions, triggers, and connectors used. This makes it more scalable for high-volume workflows, but pricing can increase with complexity.\n6. Use Cases:\nPower Automate: Typically used for business-focused workflows like automating approval processes, synchronizing data between apps, generating reports, or automating tasks within Microsoft 365.\nAzure Logic Apps: Often used for enterprise integration scenarios like integrating cloud and on-premises systems, automating large-scale workflows, connecting with ERP systems, or managing APIs across multiple services.\nSummary of Differences:\nFeature Power Automate  Azure Logic Apps\nAudience    Business users, citizen developers  Developers, IT professionals\nPlatform    Power Platform (Microsoft 365 focus)    Azure (scalable, enterprise focus)\nDevelopment No-code/Low-code, simple interface  Advanced, customizable, integrates with Azure\nIntegration Focus on Microsoft 365, business apps   Extensive integrations, including Azure services and on-prem systems\nPricing Per-user, per-flow pricing  Pay-per-action pricing, scalable\nUse Case    Automating business tasks (approvals, notifications, simple flows)  Enterprise-grade workflows, system integrations, cloud/on-prem automation\nWhen to Use Each:\nPower Automate: Best for automating day-to-day business tasks with a simple interface and deep integration into Microsoft 365.\nAzure Logic Apps: Ideal for complex, large-scale workflows, especially when integrating with Azure services or when enterprise-level scalability is needed.\nIn summary, Power Automate is the preferred tool for simpler, business-centric workflows, while Azure Logic Apps is designed for more complex, scalable, and enterprise-grade workflows. Both can be used to integrate with a bot, depending on the complexity of the automation needed\n\n\n\n\n\n</code></pre>"},{"location":"IntelligentOrchestration/Bots/#ai-agent-for-automation-content","title":"AI Agent for Automation Content","text":"<pre><code>\nAI agents can both complement and, in some cases, override the need for traditional tools like Power Automate or Azure Logic Apps, depending on the complexity of the use case and the intelligence required. Let\u2019s break it down:\n\n1. Complementing Workflows (Power Automate and Logic Apps):\nAI agents can work in tandem with these workflow automation tools by enhancing their capabilities with intelligence and decision-making. Here\u2019s how AI agents can complement workflows:\n\nDecision Making and Personalization:\nWhile tools like Power Automate and Logic Apps are fantastic for automating structured workflows based on predefined conditions, they often lack the dynamic decision-making that AI agents bring. For example, an AI agent can personalize responses, make real-time decisions, or adapt workflows based on user behavior, preferences, or changing data.\nIn a scenario where predefined conditions might be too rigid, an AI agent can assess the context (via natural language processing or other AI services) and then invoke the appropriate workflow based on more nuanced logic.\nAdvanced Interactions:\nAI agents enable more natural, human-like interactions. For example, while a Power Automate flow might trigger an action when a user fills out a form, an AI agent could carry on a conversation with the user, ask clarifying questions, and then choose which automated workflow to trigger based on deeper understanding (via Azure Cognitive Services).\nTriggering Workflows:\nAI agents can trigger Power Automate or Logic Apps workflows based on user interactions or real-time data. The agent could assess incoming requests and determine which backend workflows should run, whether they are simple tasks like sending notifications or more complex processes like updating an ERP system.\n2. Overriding Traditional Tools: When AI Agents Become the Primary Driver:\nIn some cases, AI agents can essentially override or reduce reliance on tools like Power Automate or Logic Apps, particularly when flexibility, adaptability, and intelligence are key to the process. Here\u2019s when AI agents could take the lead:\n\nUnstructured Workflows:\n\nIn scenarios where workflows are dynamic and unstructured, traditional workflow tools may struggle because they rely on rigid, predefined rules. AI agents, however, can learn, adapt, and make decisions on the fly based on real-time inputs. This is particularly useful in complex environments like customer service, where every interaction might be different, requiring the agent to dynamically adjust the process.\nCognitive Automation:\n\nAI agents can take over processes that require understanding, learning, and decision-making, far beyond what traditional automation tools can handle. For instance, an AI agent might review contracts, analyze sentiment, and even generate responses that adapt based on context, eliminating the need for a static workflow.\nIn this sense, AI agents provide cognitive automation\u2014not just automating repetitive tasks but also understanding and acting upon the information they process.\nComplex Decision Trees:\n\nAI agents can handle complex decision trees where there are numerous branching paths, exceptions, and conditions that traditional workflow tools might struggle to manage efficiently. For example, an AI agent can use machine learning models to predict outcomes, optimize decisions, or continuously improve its behavior without the need for manually predefined workflows.\nWhen to Use AI Agents Over Workflow Tools:\nHigh Variability in Processes: If the process has many variables, conditions, and exceptions, AI agents can dynamically manage these situations better than fixed workflows.\nLearning from Data: If the process requires adaptation and learning from data, AI agents can use historical information, user behavior, or real-time inputs to evolve and improve over time.\nNatural Language Interactions: If natural language conversations are key (e.g., customer support or virtual assistants), AI agents can manage these interactions, decide what workflows to trigger, or even resolve issues without needing a traditional workflow.\nComplex Decision-Making: AI agents excel when a process involves complex decision-making, where factors like predictions, recommendations, or optimizations are needed in real-time.\nExamples of AI Agents Overriding Traditional Workflows:\nCustomer Service: Instead of triggering workflows based on rigid forms or predefined responses, an AI agent could hold dynamic conversations, understand the context, and autonomously decide whether to escalate an issue, resolve it, or gather more information\u2014without a predefined workflow.\nSupply Chain Optimization: An AI agent could analyze data from various points in the supply chain and adjust operations on the fly based on predicted demand, availability of materials, or external factors like weather or market conditions\u2014overriding the need for static workflow automation.\nDocument Processing: AI agents could read, understand, and extract key information from documents (e.g., contracts, invoices), adapt based on context, and then autonomously decide what actions to take, such as making approvals, generating summaries, or routing tasks.\nConclusion:\nAI agents can complement traditional workflow tools like Power Automate and Azure Logic Apps by adding intelligence, adaptability, and dynamic decision-making to predefined workflows.\nIn more complex, unstructured, or cognitive scenarios, AI agents can override traditional workflows entirely, becoming the primary driver for automation and decision-making processes by understanding context, learning from data, and adapting in real time.\nThus, while traditional tools are great for predictable, rule-based automations, AI agents shine when tasks require intelligence, adaptability, and learning.\n\n\n</code></pre>"},{"location":"IntelligentOrchestration/Index/","title":"Agent-Based Automation Index","text":"<p>Coming soon!</p> <p></p>"},{"location":"Introduction/ApplicationEvolution/","title":"The Application Evolution","text":"<p>Blah, blah, blah</p> <p>Figure 1.1 visually depicts the evolution from traditional static apps to dynamic intelligent apps.</p> <p></p> <p>Figure 1-1. The Intelligent Apps Evolution</p> <p>Blah, blah, blah</p> <ul> <li>blah</li> <li>blah</li> <li>blah</li> <li>blah</li> <li>blah</li> </ul> <p>Blah, blah, blah</p>"},{"location":"Introduction/ApplicationEvolution/#blah-blah-blah","title":"Blah, blah blah","text":"<p>So, what exactly is Cloud Native? Sit back, relax, and let us help you explore this new world.</p>"},{"location":"Introduction/introduction/","title":"Introduction","text":"<p>Another day, at the office, working on \"the next big thing.\"</p> <p>Your cellphone rings. It's your friendly recruiter - the one who calls daily with exciting new opportunities.</p> <p>But this time it's different: Start-up, equity, and plenty of funding.</p> <p>The mention of the cloud, microservices, and cutting-edge technology pushes you over the edge.</p> <p>Fast forward a few weeks and you're now a new employee in a design session architecting a major eCommerce application. You're going to compete with the leading eCommerce sites.</p> <p>How will you build it?</p> <p>If you follow the guidance from past 15 years, you'll most likely build the system shown in Figure 1.1.</p> <p></p> <p>Figure 1-1. Traditional monolithic design</p> <p>You construct a large core application containing all of your domain logic. It includes modules such as Identity, Catalog, Ordering, and more. They directly communicate with each other within a single server process. The modules share a large relational database. The core exposes functionality via an HTML interface and a mobile app.</p> <p>Congratulations!  You just created a monolithic application.</p> <p>Not all is bad. Monoliths offer some distinct advantages. For example, they're straightforward to...</p> <ul> <li>build</li> <li>test</li> <li>deploy</li> <li>troubleshoot</li> <li>vertically scale</li> </ul> <p>Many successful apps that exist today were created as monoliths. The app is a hit and continues to evolve, iteration after iteration, adding more functionality.</p> <p>At some point, however, you begin to feel uncomfortable. You find yourself losing control of the application. As time goes on, the feeling becomes more intense, and you eventually enter a state known as the <code>Fear Cycle</code>:</p> <ul> <li>The app has become so overwhelmingly complicated that no single person understands it.</li> <li>You fear making changes - each change has unintended and costly side effects.</li> <li>New features/fixes become tricky, time-consuming, and expensive to implement.</li> <li>Each release becomes as small as possible and requires a full deployment of the entire application.</li> <li>One unstable component can crash the entire system.</li> <li>New technologies and frameworks aren't an option.</li> <li>It's difficult to implement agile delivery methodologies.</li> <li>Architectural erosion sets in as the code base deteriorates with never-ending \"quick fixes.\"</li> <li>Finally, the consultants come in and tell you to rewrite it.</li> </ul> <p>Sound familiar?</p> <p>Many organizations have addressed this monolithic fear cycle by adopting a cloud-native approach to building systems. Figure 1-2 shows the same system built applying cloud-native techniques and practices.</p> <p></p> <p>Figure 1-2. Cloud-native design</p> <p>Note how the application is decomposed across a set of small isolated microservices. Each service is self-contained and encapsulates its own code, data, and dependencies. Each is deployed in a software container and managed by a container orchestrator. Instead of a large relational database, each service owns it own datastore, the type of which vary based upon the data needs. Note how some services depend on a relational database, but other on NoSQL databases. One service stores its state in a distributed cache. Note how all traffic routes through an API Gateway service that is responsible for routing traffic to the core back-end services and enforcing many cross-cutting concerns. Most importantly, the application takes full advantage of the scalability, availability, and resiliency features found in modern cloud platforms.</p>"},{"location":"Introduction/introduction/#cloud-native-computing","title":"Cloud-native computing","text":"<p>Hmm... We just used the term, Cloud Native. Your first thought might be, \"What exactly does that mean?\" Another industry buzzword concocted by software vendors to market more stuff?\"</p> <p>Fortunately it's far different, and hopefully this book will help convince you.</p> <p>Within a short time, cloud native has become a driving trend in the software industry. It's a new way to construct large, complex systems. The approach takes full advantage of modern software development practices, technologies, and cloud infrastructure. Cloud native changes the way you design, implement, deploy, and operationalize systems.</p> <p>Unlike the continuous hype that drives our industry, cloud native is for-real. Consider the Cloud Native Computing Foundation (CNCF), a consortium of over 400 major corporations. Its charter is to make cloud-native computing ubiquitous across technology and cloud stacks. As one of the most influential open-source groups, it hosts many of the fastest-growing open source-projects in GitHub. These projects include Kubernetes, Prometheus, Helm, Envoy, and gRPC.</p> <p>The CNCF fosters an ecosystem of open-source and vendor-neutrality. Following that lead, this book presents cloud-native principles, patterns, and best practices that are technology agnostic. At the same time, we discuss the services and infrastructure available in the Microsoft Azure cloud for constructing cloud-native systems.</p> <p>So, what exactly is Cloud Native? Sit back, relax, and let us help you explore this new world.</p>"},{"location":"Other/Frameworks/Langchain/","title":"Langchain","text":""},{"location":"Other/Frameworks/Langchain/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Other/Frameworks/LlamaIndex/","title":"Llama Index","text":""},{"location":"Other/Frameworks/LlamaIndex/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Other/Frameworks/SemanticKernel/","title":"Semantic Kernel","text":""},{"location":"Other/Frameworks/SemanticKernel/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p> <p></p>"},{"location":"Platform/OpenAIModels/","title":"OpenAI Models","text":""},{"location":"Platform/OpenAIModels/#overview-of-azure-ai-services-content","title":"Overview of Azure AI Services Content","text":""},{"location":"Platform/OpenAIModels/#index-page-for-platformazure-ai-services-section","title":"Index Page for Platform/Azure AI Services Section","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/","title":"AI Services Overview","text":""},{"location":"Platform/AzureAIServices/#overview-of-azure-ai-services-content","title":"Overview of Azure AI Services Content","text":""},{"location":"Platform/AzureAIServices/#index-page-for-platformazure-ai-services-section","title":"Index Page for Platform/Azure AI Services Section","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/documentIntelligence/","title":"Document Intelligence","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/language/","title":"AI Language","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/translator/","title":"AI Translator","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/vision/","title":"AI Vision","text":"<p>Coming soon!</p> <p></p>"},{"location":"Platform/AzureAIServices/vision/#ocr-capabilties","title":"OCR Capabilties","text":"<p>Optical Character Recognition (OCR) is a specific technology used to convert different types of documents, such as scanned paper documents, PDF files, or images captured by a digital camera, into editable and searchable data. </p> <p>While OCR can be part of a broader computer vision toolkit, it focuses specifically on text extraction.</p> <p>Azure Cognitive Services provides an OCR API as part of its Computer Vision service. This OCR capability can be used in conjunction with other computer vision tools to enhance the image analysis process.</p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page2/","title":"Page 2","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page2/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page3/","title":"Page 3","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page3/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page4/","title":"Page 2","text":""},{"location":"Tools/Unlocking%20AI%20-%20An%20Introduction/page4/#another-heading","title":"Another Heading","text":"<p>Some sample text ```</p>"}]}